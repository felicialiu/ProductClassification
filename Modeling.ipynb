{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training\n",
    "\n",
    "Author: Felicia Liu\n",
    "\n",
    "Date: August 14, 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "import json\n",
    "import pprint\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import textacy\n",
    "from textacy import preprocess\n",
    "from textacy.vsm import vectorizers\n",
    "import textacy.tm\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.semi_supervised import label_propagation\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('product_data_handcorrect_2_100.json', 'r') as file:\n",
    "    data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(product_descriptions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve all data (100 items hand-labeled, 900 items pre-labeled with topic modeling)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_descriptions = []\n",
    "raw_labels = []\n",
    "for index, product in enumerate(data):\n",
    "    if 'description' in product:\n",
    "        product_descriptions.append(product['description'])\n",
    "    else:\n",
    "        print(\"Missing description for product {}\".format(index))\n",
    "    if 'label' in product:\n",
    "        raw_labels.append(product['label'])\n",
    "    else:\n",
    "        print(\"Missing label\")\n",
    "        raw_labels.append('Other')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping from category to integer\n",
    "label_mapping = {\"Dress\": 0, \"Dresses\": 0, \"Tops\": 1, \"Jeans\": 2, \"Skirts\": 3, \"Rompers\": 4, \"Shoes\": 5, \"Bags\": 6, \"Jewelry\": 7, \" Jewelry\": 7, \"Swimwear\": 8, \"Intimates\": 9, \"Other\": 10, \"Accessories\": 10, \" Accessories\": 10, '': 10}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_labels = [label_mapping[label] for label in raw_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_product_descriptions = product_descriptions[:100]\n",
    "hand_labels = num_labels[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "rest_product_descriptions = product_descriptions[100:]\n",
    "tm_labels = num_labels[100:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(0)\n",
    "indices = np.arange(len(rest_product_descriptions))\n",
    "rng.shuffle(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_total_samples = len(num_labels)\n",
    "n_labeled_points = 100\n",
    "max_iterations = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "unlabeled_indices = np.arange(n_total_samples)[n_labeled_points:]\n",
    "f = plt.figure()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert text data into vector representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_descriptions(descriptions, filter_words):\n",
    "    preprocessed_descriptions = []\n",
    "    for description in descriptions:\n",
    "        if bool(BeautifulSoup(description, \"html.parser\").find()):\n",
    "            soup = BeautifulSoup(description)\n",
    "            description = soup.text\n",
    "        preprocessed = description.replace('\\n', ' ')\n",
    "        preprocessed = preprocessed.lower()\n",
    "        for word in filter_words:\n",
    "            preprocessed = preprocessed.replace(word.lower(), ' ')\n",
    "        preprocessed = preprocess.normalize_whitespace(preprocessed)\n",
    "        preprocessed_descriptions.append(preprocessed)\n",
    "    return preprocessed_descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_words = ['J.Crew', 'shipping', 'free', 'available', 'entire', 'selection']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_descriptions = preprocess_descriptions(product_descriptions, filter_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_text(input_texts):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    vectors = vectorizer.fit_transform(input_texts)\n",
    "    print(vectors.shape)\n",
    "    return vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 5281)\n"
     ]
    }
   ],
   "source": [
    "vectors = vectorize_text(preprocessed_descriptions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying label spreading\n",
    "\n",
    "Spent a lot of time to try to make this work, but unfortunately ran out of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000,)\n",
      "(1000, 5281)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 330 is out of bounds for axis 0 with size 330",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-122-12d28ca386d5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mpredicted_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlp_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransduction_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0munlabeled_indices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mtrue_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0munlabeled_indices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mcm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrue_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlp_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 330 is out of bounds for axis 0 with size 330"
     ]
    }
   ],
   "source": [
    "for i in range(max_iterations):\n",
    "    if len(unlabeled_indices) == 0:\n",
    "        print(\"No unlabeled items left to labe.\")\n",
    "        break\n",
    "    y_train = np.copy(num_labels)\n",
    "    y_train[unlabeled_indices] = -1\n",
    "    y_train = y_train\n",
    "    print(y_train.shape)\n",
    "    lp_model = label_propagation.LabelSpreading(gamma=0.25, max_iter=20)\n",
    "    print(vectors.shape)\n",
    "    vectors = vectors.toarray()\n",
    "    lp_model.fit(vectors, y_train)\n",
    "    \n",
    "    predicted_labels = lp_model.transduction_[unlabeled_indices]\n",
    "    true_labels = y[unlabeled_indices]\n",
    "    \n",
    "    cm = confusion_matrix(true_labels, predicted_labels, labels=lp_model.classes_)\n",
    "    \n",
    "    print(classification_report(true_labels, predicted_labels))\n",
    "    print(cm)\n",
    "    \n",
    "    predicted_entropies = stats.distributions.entropy(lp_model.label_distributions_.T)\n",
    "    uncertainty_ind = np.argsort(predicted_entropies)[::-1]\n",
    "    uncertainty_ind = uncertainty_ind[np.in1d(uncertainty_index, unlabeled_indices)][:5]\n",
    "    # ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline logistic regression model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train with 100 hand-labeled data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_labeled = preprocess_descriptions(labeled_product_descriptions, filter_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "vectors_labeled = vectorizer.fit_transform(preprocessed_labeled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorize the remaining 900 data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_rest = preprocess_descriptions(rest_product_descriptions, filter_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors_rest = vectorizer.transform(preprocessed_rest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/felicialiu/.pyenv/versions/3.6.7/envs/data-science-env/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/felicialiu/.pyenv/versions/3.6.7/envs/data-science-env/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LogisticRegression()\n",
    "model.fit(vectors_labeled, hand_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Produce predictions and write to json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(vectors_rest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_labels_final = list(hand_labels) + list(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_mapping = {\"Dress\": 0, \"Dresses\": 0, \"Tops\": 1, \"Jeans\": 2, \"Skirts\": 3, \"Rompers\": 4, \"Shoes\": 5, \"Bags\": 6, \"Jewelry\": 7, \" Jewelry\": 7, \"Swimwear\": 8, \"Intimates\": 9, \"Other\": 10, \"Accessories\": 10, \" Accessories\": 10, '': 10}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_mapping = dict([[v, k] for k,v in label_mapping.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_mapping[7] = 'Jewelry'\n",
    "reverse_mapping[10] = 'Other'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_copy = deepcopy(data)\n",
    "\n",
    "for product_data, label in zip(data_copy, all_labels_final):\n",
    "    product_data['label'] = reverse_mapping[label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('product_data_result.json', 'w') as outfile:\n",
    "    json.dump(data_copy, outfile, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
